---
layout:     post
title:      "中文分词调研"
subtitle:   "中文分词原理介绍，分词工具介绍"
date:       2018-05-18 11:13:00
author:     "JerryShao"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - nlp
    - 分词
---

# 什么是中文分词
中文分词（chinese word segmetation）指的是将一个汉字序列分割成一个个单独的词的过程。

# 中文分词方法
中文分词基于实现原理和特点，主要分为以下2类：
** 基于词典的分词算法
  也称字符串匹配分词算法，或机械分词算法。该算法是按照一定的策略将待匹配的字符串和一个已建立好的“充分大的”词典中的词进行匹配，若找到某个词条，则说明匹配成功，识别了该词。按照扫描方向的不同，字符串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，可以分为单纯分词方法和分词与词性标注相结合的一体化方法。常见的基于词典的分词算法分为以下几种：
** 正向最大匹配法（从左到右的方向）
** 逆向最大匹配法（从右到左的方向）
** 双向最大匹配法（进行从左到右，从右到左两次扫描）
** 最小切分（每一句中切出的词数量最少）

  基于词典的分词算法是应用最广泛、分词速度最快的。很长一段时间内研究者都在对基于字符串匹配方法进行优化，比如最大长度设定、字符串存储和查找方式以及对于词表的组织结构，比如采用TRIE索引树、哈希索引等。

** 基于理解的分词方法
  基于理解的分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。

** 基于统计的分词方法
  基于统计的分词方法是在给定大量已经分词的文本的前提下，利用统计机器学习模型学习词语切分的规律（称为训练），从而实现对未知文本的切分。例如最大概率分词方法和最大熵分词方法等。随着大规模语料库的建立，统计机器学习方法的研究和发展，基于统计的中文分词方法渐渐成为了主流方法

  主要的统计模型有：N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model ，HMM），最大熵模型（ME），条件随机场模型（Conditional Random Fields，CRF),SVM, 深度学习等。比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。

  在实际的应用中，基于统计的分词系统都需要使用分词词典来进行字符串匹配分词，同时使用统计方法识别一些新词，即将字符串频率统计和字符串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。
  Nianwen Xue在其论文《Combining Classifiers for Chinese Word Segmentation》中首次提出对每个字符进行标注，通过机器学习算法训练分类器进行分词，在论文《Chinese word segmentation as character tagging》中较为详细地阐述了基于字标注的分词法。
  随着深度学习的兴起，也出现了基于神经网络的分词器，例如有人员尝试使用双向LSTM+CRF实现分词器，其本质上是序列标注，所以有通用性，命名实体识别等都可以使用该模型，据报道其分词器字符准确率可高达97.5%。算法框架的思路与论文《Neural Architectures forNamed Entity Recognition》类似，利用该框架可以实现中文分词，如下图所示：
&amp;lt;img src=&quot;https://pic2.zhimg.com/v2-aad7ef8156b33c51efeb0f7f4b6f614d_b.jpg&quot; data-rawwidth=&quot;633&quot; data-rawheight=&quot;498&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;633&quot; data-original=&quot;https://pic2.zhimg.com/v2-aad7ef8156b33c51efeb0f7f4b6f614d_r.jpg&quot;&amp;gt;
首先对语料进行字符嵌入，将得到的特征输入给双向LSTM，然后加一个CRF就得到标注结果。
 
# 分词器当前存在的问题

目前中文分词难点主要有三个：

1、分词标准：比如人名，在哈工大的标准中姓和名是分开的，但在Hanlp中是合在一起的。这需要根据不同的需求制定不同的分词标准。

2、歧义：对同一个待切分字符串存在多个分词结果。

歧义又分为组合型歧义、交集型歧义和真歧义三种类型。

1) 组合型歧义：分词是有不同的粒度的，指某个词条中的一部分也可以切分为一个独立的词条。比如“中华人民共和国”，粗粒度的分词就是“中华人民共和国”，细粒度的分词可能是“中华/人民/共和国”

2) 交集型歧义：在“郑州天和服装厂”中，“天和”是厂名，是一个专有词，“和服”也是一个词，它们共用了“和”字。

3) 真歧义：本身的语法和语义都没有问题, 即便采用人工切分也会产生同样的歧义，只有通过上下文的语义环境才能给出正确的切分结果。例如：对于句子“美国会通过对台售武法案”，既可以切分成“美国/会/通过对台售武法案”，又可以切分成“美/国会/通过对台售武法案”。

一般在搜索引擎中，构建索引时和查询时会使用不同的分词算法。常用的方案是，在索引的时候使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。

3、新词：也称未被词典收录的词，该问题的解决依赖于人们对分词技术和汉语语言结构的进一步认识。


# 中文分词工具介绍
